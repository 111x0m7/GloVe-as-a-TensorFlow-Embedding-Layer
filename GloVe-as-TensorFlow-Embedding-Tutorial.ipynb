{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe as a TensorFlow Embedding layer\n",
    "\n",
    "In this tutorial, we'll see how to convert GloVe embeddings to TensorFlow layers. This could also work with embeddings generated from word2vec.\n",
    "\n",
    "First, we'll download the embedding we need. \n",
    "\n",
    "Second, we'll load it into TensorFlow to convert input words with the embedding to word features. The conversion is done within TensorFlow, so it is GPU-optimized and it could run on batches on the GPU. It is also possible to run this tutorial with just a CPU. \n",
    "\n",
    "What you'll need: \n",
    "- A working installation of TensorFlow.\n",
    "- 4 to 6 GB of disk space to download embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, some theory\n",
    "\n",
    "### Representations\n",
    "\n",
    "We need a way to represent content in neural networks. For audio, it's possible to use a [spectrogram](https://github.com/guillaume-chevalier/filtering-stft-and-laplace-transform). For images, it's possible to directly use the pixels and then get features maps from a convolutional neural network. For text, analyzing every letter is costly, so it's better to use word representations to embed words or documents as vectors into Artificial Neural Networks and other Machine Learning algorithms. \n",
    "> ![Features from content](https://www.tensorflow.org/images/audio-image-text.png)\n",
    "> https://www.tensorflow.org/tutorials/word2vec\n",
    "\n",
    "As described by Keras, an embedding:\n",
    "\n",
    "> \"Turns positive integers (indexes) into dense vectors of fixed size\".\n",
    "\n",
    "That's it. It's to extract features from words. An embedding is a huge matrix for which each row is a word, and each column is a feature from that word. To summarize, it's possible to convert a word to a vector of a certain length, such as 25, or 100, 200, 1000, and on. In practice, a length of 100 to 300 features is acceptable. With less than 100, we would risk underfitting our linguistic dataset. Word embeddings can eat a lot of RAM, so in this tutorial we'll download and use dimensions of size 25, however changing that to 200 would be a breeze with the actual code. \n",
    "\n",
    "### You can compute word analogies\n",
    "\n",
    "The word representations (features) are linear, therefore it's possible to add and substract words with word embeddings. For example, here's the most known word analogy example:\n",
    "\n",
    "$$\\text{King} - \\text{Man} = \\text{Queen} - \\text{Woman}$$\n",
    "$$\\Longleftrightarrow$$\n",
    "$$\\text{King} - \\text{Man} + \\text{Woman} = \\text{Queen}$$\n",
    "\n",
    " For example, it's possible to change from: \n",
    "- Masculine and feminine\n",
    "- Country and capital\n",
    "- Singular and plural\n",
    "- Verb tenses\n",
    "- And the list goes on...\n",
    "\n",
    "> ![Word features from embeddings](https://www.tensorflow.org/images/linear-relationships.png)\n",
    "> https://www.tensorflow.org/tutorials/word2vec\n",
    "\n",
    "It's also possible to compute the [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between a word A and a word B, which is the cosine of the angle between the two words. A cosine similarity of -1 would mean the words are complete opposites, while a cosine similarity of 1 would mean that the words are the same. Here's the formula to compare two words: \n",
    "\n",
    "$$\\text{Cosine Similarity}=cos({\\theta}_{AB})=\\frac{A \\cdot B}{|A|_2 |B|_2}$$\n",
    "\n",
    "Here, the norm (such as ${|A|_2}$) is the **L2 norm**, the radius in space from the origin, but in a higher dimensional space such as with $n=300$: \n",
    "\n",
    "$$|A|_2=\\sqrt{A_1 + A_2 + A_3 + ... + A_n}$$\n",
    "\n",
    "### How does it looks like concretely?\n",
    "\n",
    "For example, here are some cosine similarities computed from the code explained below: \n",
    "\n",
    "| Word A | Word B | Cosine Similarity |\n",
    "| ------------- | ------------- | ----- |\n",
    "| 1 | right-aligned | 1600 |\n",
    "| 1 | centered | 12 |\n",
    "| 1 | are neat | 1 |\n",
    "\n",
    "Finally, notice how similar words are close in space: \n",
    "> ![](https://www.tensorflow.org/images/embedding-nearest-points.png)\n",
    "> https://www.tensorflow.org/programmers_guide/embedding\n",
    "\n",
    "Note: in the image above, the embedding have been subsampled to a lower 3D space with a PCA (Princial Component Analysis) or t-SNE (t-distributed Stochastic Neighbor Embedding) to be explorable. This is possible with TensorBoard for inspection. 300 dimensions can't be visualised easily. \n",
    "\n",
    "### But how are those word representations obtained?\n",
    "\n",
    "Before continuing to the practical part where we'll use pretrained embeddings, it's a good thing to know that embeddings can be obtained from unsupervised training on large datasets of text. That's at least a way we can use the text off the internet! To perform this training to get an embedding, it's possible to go with the word2vec approach, or also with the GloVe (Global word Vectors). GloVe is a more recent approach that builds upon the theory of word2vec. Here, we'll use GloVe embeddings. To summarize how the unsupervised training happens, let's see what John Rupert Firth has to say: \n",
    "\n",
    "> You shall know a word by the company it keeps (Firth, J. R., 1957)\n",
    "\n",
    "It's amazing that by comparing words and trying to guess the surrounding words, it's possible to find their meaning. To learn more on that, I'd recommend you the [5th course of the Deep Learning Specialization](https://www.coursera.org/learn/nlp-sequence-models) on coursera by Andrew Ng, a course which can lead to the Deep Learning Specialization [certificate](https://www.coursera.org/account/accomplishments/specialization/U7VNC3ZD9YD8). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get practical! \n",
    "\n",
    "### First, download the pretrained embeddings with the code below\n",
    "\n",
    "Careful, the download will take 4-6 GB on disks. If you have already downloaded the embeddings, they will be located under the `./embeddings/` folder relative to here, and won't be downloaded again. \n",
    "\n",
    "Note: several embeddings were downloaded with different dimension sizes in the zip file, but we only need one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import chakin\n",
    "\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings already downloaded.\n",
      "Embeddings already extracted.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Downloading Twitter.25d embeddings from Stanford:\n",
    "\n",
    "CHAKIN_INDEX = 17\n",
    "SUBFOLDER_NAME = \"glove.twitter.27B\"\n",
    "DATA_FOLDER = \"embeddings\"\n",
    "ZIP_FILE = os.path.join(DATA_FOLDER, \"{}.zip\".format(SUBFOLDER_NAME))\n",
    "UNZIP_FOLDER = os.path.join(DATA_FOLDER, SUBFOLDER_NAME)\n",
    "\n",
    "if not os.path.exists(ZIP_FILE) and not os.path.exists(UNZIP_FOLDER):\n",
    "    # GloVe by Stanford is licensed Apache 2.0: \n",
    "    #     https://github.com/stanfordnlp/GloVe/blob/master/LICENSE\n",
    "    #     http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "    #     Copyright 2014 The Board of Trustees of The Leland Stanford Junior University\n",
    "    print(\"Downloading embeddings to '{}'\".format(ZIP_FILE))\n",
    "    chakin.search(lang='English')\n",
    "    chakin.download(number=CHAKIN_INDEX, save_dir='./{}'.format(DATA_FOLDER))\n",
    "else:\n",
    "    print(\"Embeddings already downloaded.\")\n",
    "    \n",
    "if not os.path.exists(UNZIP_FOLDER):\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(ZIP_FILE,\"r\") as zip_ref:\n",
    "        print(\"Extracting embeddings to '{}'\".format(UNZIP_FOLDER))\n",
    "        zip_ref.extractall(UNZIP_FOLDER)\n",
    "else:\n",
    "    print(\"Embeddings already extracted.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's read the embedding from disks here\n",
    "\n",
    "First, we load the embeddings, then we demonstrate their usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_embedding_from_disks(glove_filename, with_indexes=True):\n",
    "    \"\"\"\n",
    "    Read a GloVe txt file. If `with_indexes=True`, we return a tuple of two dictionnaries\n",
    "    `(word_to_index_dict, index_to_embedding_array)`, otherwise we return only a direct \n",
    "    `word_to_embedding_dict` dictionnary mapping from a string to a numpy array.\n",
    "    \"\"\"\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()\n",
    "        index_to_embedding_array = []\n",
    "    else:\n",
    "        word_to_embedding_dict = dict()\n",
    "\n",
    "    \n",
    "    with open(glove_filename, 'r') as glove_file:\n",
    "        for (i, line) in enumerate(glove_file):\n",
    "            \n",
    "            split = line.split(' ')\n",
    "            \n",
    "            word = split[0]\n",
    "            \n",
    "            representation = split[1:]\n",
    "            representation = np.array(\n",
    "                [float(val) for val in representation]\n",
    "            )\n",
    "            \n",
    "            if with_indexes:\n",
    "                word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representation\n",
    "\n",
    "    _WORD_NOT_FOUND = [0.0]* len(representation)  # Empty representation for unknown words.\n",
    "    if with_indexes:\n",
    "        _LAST_INDEX = i + 1\n",
    "        word_to_index_dict = defaultdict(lambda: _LAST_INDEX, word_to_index_dict)\n",
    "        index_to_embedding_array = np.array(index_to_embedding_array + [_WORD_NOT_FOUND])\n",
    "        return word_to_index_dict, index_to_embedding_array\n",
    "    else:\n",
    "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "        return word_to_embedding_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding from disks...\n",
      "Embedding loaded from disks.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NUMBER_OF_DIMENSIONS = 25  # Available: 25, 50, 100, 200.\n",
    "glove_filename_25d = os.path.join(UNZIP_FOLDER, \"{}.{}d.txt\".format(SUBFOLDER_NAME, NUMBER_OF_DIMENSIONS))\n",
    "\n",
    "print(\"Loading embedding from disks...\")\n",
    "word_to_index, index_to_embedding = load_embedding_from_disks(glove_filename_25d, with_indexes=True)\n",
    "print(\"Embedding loaded from disks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unknown words have representations with values of zero, such as [0, 0, ..., 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding is of shape: (1193515, 25)\n",
      "This means (number of words, number of dimensions per word)\n",
      "\n",
      "The first words are words that tend occur more often.\n",
      "Note: for unknown words, the representation is an empty vector,\n",
      "and the index is the last one. The dictionnary has a limit:\n",
      "    A word --> Index in embedding --> Representation\n",
      "    worsdfkljsdf --> 1193514 --> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "    the --> 13 --> [-0.010167, 0.020194, 0.21473, 0.17289, -0.43659, -0.14687, 1.8429, -0.15753, 0.18187, -0.31782, 0.06839, 0.51776, -6.3371, 0.48066, 0.13777, -0.48568, 0.39, -0.0019506, -0.10218, 0.21262, -0.86146, 0.17263, 0.18783, -0.8425, -0.31208]\n"
     ]
    }
   ],
   "source": [
    "vocab_size, embedding_dim = index_to_embedding.shape\n",
    "print(\"Embedding is of shape: {}\".format(index_to_embedding.shape))\n",
    "print(\"This means (number of words, number of dimensions per word)\\n\")\n",
    "print(\"The first words are words that tend occur more often.\")\n",
    "\n",
    "print(\"Note: for unknown words, the representation is an empty vector,\\n\"\n",
    "      \"and the index is the last one. The dictionnary has a limit:\")\n",
    "print(\"    {} --> {} --> {}\".format(\"A word\", \"Index in embedding\", \"Representation\"))\n",
    "word = \"worsdfkljsdf\"\n",
    "idx = word_to_index[word]\n",
    "embd = list(np.array(index_to_embedding[idx], dtype=int))  # \"int\" for compact print only.\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
    "word = \"the\"\n",
    "idx = word_to_index[word]\n",
    "embd = list(index_to_embedding[idx])  # \"int\" for compact print only.\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The L2 norm of some words can vary\n",
    "Notice how more common words have a longer embedding norm, how some text on Twitter was in French, and how it deals with punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The:           6.825211375610675\n",
      "Teh:           5.168743789022242\n",
      "A:             6.697466132368121\n",
      "It:            6.6026557827711265\n",
      "Its:           5.815954107190023\n",
      "Bacon:         5.061203156401844\n",
      "Star:          4.377212317550177\n",
      "Clone:         3.318821469851497\n",
      "Bonjour:       4.569226609068979\n",
      "Intelligence:  4.978693160848336\n",
      "À:             6.757003390719884\n",
      "A:             6.697466132368121\n",
      "Ça:            6.447278498352788\n",
      "Ca:            5.757383694621195\n",
      "Été:           5.7824019165330425\n",
      "C'est:         6.6648048720694\n",
      "Aujourd'hui:   0.0\n",
      "Aujourd:       5.28318283424017\n",
      "':             5.146860627039459\n",
      "hui:           4.813765207599868\n",
      "?:             5.291857611164723\n",
      "!:             5.145156971946508\n",
      ",:             5.401075354278071\n",
      ".:             4.965766197438418\n",
      "-:             5.10790151248338\n",
      "/:             5.063642432429447\n",
      "~:             4.910993053437055\n",
      "Note: here we printed words starting with capital letters, \n",
      "however to take their embeddings we need their lowercase version (str.lower())\n"
     ]
    }
   ],
   "source": [
    "words = [\n",
    "    \"The\", \"Teh\", \"A\", \"It\", \"Its\", \"Bacon\", \"Star\", \"Clone\", \"Bonjour\", \"Intelligence\", \n",
    "    \"À\", \"A\", \"Ça\", \"Ca\", \"Été\", \"C'est\", \"Aujourd'hui\", \"Aujourd\", \"'\", \"hui\", \"?\", \"!\", \",\", \".\", \"-\", \"/\", \"~\"\n",
    "]\n",
    "\n",
    "for word in words:\n",
    "    word_ = word.lower()\n",
    "    embedding = index_to_embedding[word_to_index[word_]]\n",
    "    norm = str(np.linalg.norm(embedding))\n",
    "    print((word + \": \").ljust(15) + norm)\n",
    "print(\"Note: here we printed words starting with capital letters, \\n\"\n",
    "      \"however to take their embeddings we need their lowercase version (str.lower())\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load the embedding in TensorFlow\n",
    "\n",
    "We simply create a non-trainable (frozen) tf.Variable() which we set to hold the value of the big embedding matrix.\n",
    "\n",
    "First, let's define the variables and graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = None  # Any size is accepted\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()  # sess = tf.Session()\n",
    "\n",
    "# Define the variable that will hold the embedding:\n",
    "tf_embedding = tf.Variable(\n",
    "    tf.constant(0.0, shape=index_to_embedding.shape),\n",
    "    trainable=False,\n",
    "    name=\"Embedding\"\n",
    ")\n",
    "\n",
    "tf_word_ids = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "\n",
    "tf_word_representation_layer = tf.nn.embedding_lookup(\n",
    "    params=tf_embedding,\n",
    "    ids=tf_word_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sending the embedding to TensorFlow below. It will be located in the GPU from now (or on CPU if GPU is unavailable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding now stored in TensorFlow. Can delete numpy array to clear some CPU RAM.\n"
     ]
    }
   ],
   "source": [
    "tf_embedding_placeholder = tf.placeholder(tf.float32, shape=index_to_embedding.shape)\n",
    "tf_embedding_init = tf_embedding.assign(tf_embedding_placeholder)\n",
    "_ = sess.run(\n",
    "    tf_embedding_init, \n",
    "    feed_dict={\n",
    "        tf_embedding_placeholder: index_to_embedding\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Embedding now stored in TensorFlow. Can delete numpy array to clear some CPU RAM.\")\n",
    "del index_to_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use or fetch representations, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representations for ['Hello', 'World', '!']:\n",
      "[[-0.77069    0.12827    0.33137    0.0050893 -0.47605   -0.50116\n",
      "   1.858      1.0624    -0.56511    0.13328   -0.41918   -0.14195\n",
      "  -2.8555    -0.57131   -0.13418   -0.44922    0.48591   -0.6479\n",
      "  -0.84238    0.61669   -0.19824   -0.57967   -0.65885    0.43928\n",
      "  -0.50473  ]\n",
      " [ 0.10301    0.095666  -0.14789   -0.22383   -0.14775   -0.11599\n",
      "   1.8513     0.24886   -0.41877   -0.20384   -0.08509    0.33246\n",
      "  -4.6946     0.84096   -0.46666   -0.031128  -0.19539   -0.037349\n",
      "   0.58949    0.13941   -0.57667   -0.44426   -0.43085   -0.52875\n",
      "   0.25855  ]\n",
      " [ 0.4049    -0.87651   -0.23362   -0.34844   -0.097002   0.40895\n",
      "   1.6928     1.7058    -1.293      0.70091   -0.12498   -0.75998\n",
      "  -3.1586     0.14081    0.57255   -0.46097   -0.75721   -0.72414\n",
      "  -1.4071    -0.17224    0.0099324 -0.45711    0.074886   1.2035\n",
      "   1.1614   ]]\n"
     ]
    }
   ],
   "source": [
    "batch_of_words = [\"Hello\", \"World\", \"!\"]\n",
    "batch_indexes = [word_to_index[w.lower()] for w in batch_of_words]\n",
    "\n",
    "embedding_from_batch_lookup = sess.run(\n",
    "    tf_word_representation_layer, \n",
    "    feed_dict={\n",
    "        tf_word_ids: batch_indexes\n",
    "    }\n",
    ")\n",
    "print(\"Representations for {}:\".format(batch_of_words))\n",
    "print(embedding_from_batch_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To avoid loading the embedding twice in RAM, make TensorFlow able to load them from disks directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF embeddings saved to 'embeddings/glove.twitter.27B.ckpt'.\n",
      "word_to_index dict saved to 'embeddings/glove.twitter.27B.json'.\n"
     ]
    }
   ],
   "source": [
    "TF_EMBEDDINGS_FILE_NAME = os.path.join(DATA_FOLDER, SUBFOLDER_NAME + \".ckpt\")\n",
    "DICT_WORD_TO_INDEX_FILE_NAME = os.path.join(DATA_FOLDER, SUBFOLDER_NAME + \".json\")\n",
    "\n",
    "variables_to_save = [tf_embedding]\n",
    "embedding_saver = tf.train.Saver(variables_to_save)\n",
    "embedding_saver.save(sess, save_path=TF_EMBEDDINGS_FILE_NAME)\n",
    "print(\"TF embeddings saved to '{}'.\".format(TF_EMBEDDINGS_FILE_NAME))\n",
    "sess.close()\n",
    "\n",
    "with open(DICT_WORD_TO_INDEX_FILE_NAME, 'w') as f:\n",
    "    json.dump(word_to_index, f)\n",
    "print(\"word_to_index dict saved to '{}'.\".format(DICT_WORD_TO_INDEX_FILE_NAME))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like absolutely crazy not hate bag sand rock soap\n",
      "[293, 10, 151, 49, 1193514, 11, 76, 137, 50, 293, 51, 187, 49, 293, 47, 1193514, 210, 73, 11, 1016, 47, 1193514, 36, 50, 187, 1193514, 369, 11, 187, 49, 1193514, 76, 11, 456, 1193514, 137, 11, 36, 199, 1193514, 73, 50, 210, 151, 1193514, 137, 50, 11, 351]\n"
     ]
    }
   ],
   "source": [
    "words_B = \"like absolutely crazy not hate bag sand rock soap\"\n",
    "r = [word_to_index[w.strip()] for w in words_B]\n",
    "print(words_B)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model to get word similarities from word A to a list of many words B\n",
    "\n",
    "This is for demo purposes. With a GPU, we can fetch many words quickly and compute on them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restarting from scratch: resetting the Jupyter notebook and loading embeddings from disks, the good way\n",
    "\n",
    "Now that we have a TensorFlow checkpoint, let's load the embedding without having to parse the txt file into NumPy in CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "# Magic iPython/Jupyter command to delete variables and restart the Python kernel\n",
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_to_index dict restored from 'embeddings/glove.twitter.27B.json'.\n",
      "INFO:tensorflow:Restoring parameters from embeddings/glove.twitter.27B.ckpt\n",
      "TF embeddings restored from 'embeddings/glove.twitter.27B.ckpt'.\n",
      "Model created.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import json\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "DATA_FOLDER = \"embeddings\"\n",
    "SUBFOLDER_NAME = \"glove.twitter.27B\"\n",
    "TF_EMBEDDING_FILE_NAME = \"{}.ckpt\".format(SUBFOLDER_NAME)\n",
    "TF_EMBEDDINGS_FILE_PATH = os.path.join(DATA_FOLDER, SUBFOLDER_NAME + \".ckpt\")\n",
    "DICT_WORD_TO_INDEX_FILE_NAME = os.path.join(DATA_FOLDER, SUBFOLDER_NAME + \".json\")\n",
    "\n",
    "batch_size = None  # Any size is accepted\n",
    "word_representations_dimensions = 25  # Embedding of size (vocab_len, 25)\n",
    "\n",
    "\n",
    "# In case you didn't do the \"%reset\" in the cell above: \n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()  # sess = tf.Session()\n",
    "\n",
    "# Input to the graph where word IDs can be sent in batch. \n",
    "tf_word_A_id = tf.placeholder(tf.int32, shape=[1])\n",
    "tf_words_B_ids = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "\n",
    "def embedding_for_words(word_A_id, words_B_ids, tf_embeddings_file_path, dict_word_to_index_file_name, nb_dims):\n",
    "    \"\"\"\n",
    "    Define the embedding tf.Variable, load it, \n",
    "    and access it from IDs with a word and a list of words.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(DICT_WORD_TO_INDEX_FILE_NAME, 'r') as f:\n",
    "        word_to_index = json.load(f)\n",
    "    _LAST_INDEX = len(word_to_index) - 2  # TODO: -2?\n",
    "    word_to_index = defaultdict(lambda: _LAST_INDEX, word_to_index)\n",
    "    print(\"word_to_index dict restored from '{}'.\".format(dict_word_to_index_file_name))\n",
    "    \n",
    "    # 1. Define the variable that will hold the embedding:\n",
    "    tf_embedding = tf.Variable(\n",
    "        tf.constant(0.0, shape=[len(word_to_index)-1, nb_dims]),\n",
    "        trainable=False,\n",
    "        name=\"Embedding\"\n",
    "    )\n",
    "\n",
    "    # 2. Restore the embedding from disks to TensorFlow, GPU (or CPU if GPU unavailable):\n",
    "    variables_to_restore = [tf_embedding]\n",
    "    embedding_saver = tf.train.Saver(variables_to_restore)\n",
    "    embedding_saver.restore(sess, save_path=tf_embeddings_file_path)\n",
    "    print(\"TF embeddings restored from '{}'.\".format(tf_embeddings_file_path))\n",
    "    \n",
    "    # 3. TensorFlow operation to get a representation from an ID and the embedding matrix.\n",
    "    tf_word_representation_A = tf.nn.embedding_lookup(\n",
    "        params=tf_embedding, ids=tf_word_A_id)\n",
    "    tf_words_representation_B = tf.nn.embedding_lookup(\n",
    "        params=tf_embedding, ids=tf_words_B_ids)\n",
    "    \n",
    "    return tf_word_representation_A, tf_words_representation_B, word_to_index\n",
    "\n",
    "def cosine_similarity_tensorflow(tf_word_representation_A, tf_words_representation_B):\n",
    "    \"\"\"\n",
    "    Returns the `cosine_similarity = cos(angle_between_a_and_b_in_space)` \n",
    "    for the two word A to all the words B.\n",
    "    The first input word must be a 1D Tensors (word_representation).\n",
    "    The second input words must be 2D Tensors (batch_size, word_representation).\n",
    "    The result is a tf tensor that must be fetched with `sess.run`.\n",
    "    \"\"\"\n",
    "    a_normalized = tf.nn.l2_normalize(tf_word_representation_A, axis=-1)\n",
    "    b_normalized = tf.nn.l2_normalize(tf_words_representation_B, axis=-1)\n",
    "    similarity = tf.reduce_sum(\n",
    "        tf.multiply(a_normalized, b_normalized), \n",
    "        axis=-1\n",
    "    )\n",
    "    return similarity\n",
    "\n",
    "tf_word_representation_A, tf_words_representation_B, word_to_index = embedding_for_words(\n",
    "    tf_word_A_id, \n",
    "    tf_words_B_ids,\n",
    "    TF_EMBEDDINGS_FILE_PATH, \n",
    "    DICT_WORD_TO_INDEX_FILE_NAME, \n",
    "    word_representations_dimensions)\n",
    "\n",
    "cosine_similarities = cosine_similarity_tensorflow(\n",
    "    tf_word_representation_A, \n",
    "    tf_words_representation_B)\n",
    "\n",
    "print(\"Model created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the fetch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sentence_to_word_ids(sentence, word_to_index):\n",
    "    \"\"\"\n",
    "    Note: there might be a better way to split sentences for GloVe.\n",
    "    Please look at the documentation or open an issue to suggest a fix.\n",
    "    \"\"\"\n",
    "    # Separating punctuation from words:\n",
    "    for punctuation_character in punctuation:\n",
    "        sentence = sentence.replace(punctuation_character, \" {} \".format(punctuation_character))\n",
    "    # Removing double spaces and lowercasing:\n",
    "    sentence = sentence.replace(\"  \", \" \").replace(\"  \", \" \").lower().strip()\n",
    "    # Splitting on every space:\n",
    "    split_sentence = sentence.split(\" \")\n",
    "    # Converting to IDs:\n",
    "    ids = [word_to_index[w.strip()] for w in split_sentence]\n",
    "    return ids, split_sentence\n",
    "\n",
    "def predict_cosine_similarities(sess, word_A, words_B):\n",
    "    \"\"\"\n",
    "    Use the model in sess to predict cosine similarities.\n",
    "    \"\"\"\n",
    "\n",
    "    word_A_id, _ = sentence_to_word_ids(word_A, word_to_index)\n",
    "    words_B_ids, split_sentence = sentence_to_word_ids(words_B, word_to_index)\n",
    "\n",
    "    evaluated_cos_similarities = sess.run(\n",
    "        cosine_similarities, \n",
    "        feed_dict={\n",
    "            tf_word_A_id: word_A_id,\n",
    "            tf_words_B_ids: words_B_ids\n",
    "        }\n",
    "    )\n",
    "    return evaluated_cos_similarities, split_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarities with \"Science\":\n",
      "    hello:         0.4928313195705414\n",
      "    internet:      0.6569848656654358\n",
      "    ,:             0.49928268790245056\n",
      "    a:             0.4862615764141083\n",
      "    vocano:        0.0\n",
      "    erupt:         0.16974501311779022\n",
      "    like:          0.6019276976585388\n",
      "    the:           0.7420801520347595\n",
      "    bitcoin:       0.5125284194946289\n",
      "    out:           0.6307196617126465\n",
      "    of:            0.7674074172973633\n",
      "    the:           0.7420801520347595\n",
      "    blue:          0.4629560112953186\n",
      "    and:           0.6889371275901794\n",
      "    there:         0.7172714471817017\n",
      "    is:            0.7015751600265503\n",
      "    an:            0.6883363723754883\n",
      "    unknownword00: 0.0\n",
      "    !:             0.3991800844669342\n"
     ]
    }
   ],
   "source": [
    "word_A = \"Science\"\n",
    "words_B = \"Hello internet, a vocano erupt like the bitcoin out of the blue and there is an unknownWord00!\"\n",
    "\n",
    "evaluated_cos_similarities, splitted = predict_cosine_similarities(sess, word_A, words_B)\n",
    "\n",
    "print(\"Cosine similarities with \\\"{}\\\":\".format(word_A))\n",
    "for word, similarity in zip(splitted, evaluated_cos_similarities):\n",
    "    print(\"    {}{}\".format((word+\":\").ljust(15), similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  What's next?\n",
    "\n",
    "I think getting the embeddings into TensorFlow is a good step into building a language model. You may want to grab some data, such as [here](https://github.com/awesomedata/awesome-public-datasets#naturallanguage) and [here](https://github.com/niderhoff/nlp-datasets), and learn more about how recurrent neural networks can read features such as sentences or signal of varying length, such as [an LSTM (RNN) encoder](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition) or an [end-of-sentence predictor from a seq2seq GRU (RNN)](https://github.com/guillaume-chevalier/seq2seq-signal-prediction). A bidirectional RNN stacked on a CNN would be an interesting starting point. Would you build one now?\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "The pretrained word vectors can be found there: \n",
    "- Repo https://github.com/stanfordnlp/GloVe\n",
    "- Manual download: http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "\n",
    "Chakin was used to download those word embeddings: \n",
    "- https://github.com/chakki-works/chakin\n",
    "\n",
    "Some images in this notebook are references/links from the TensorFlow website: \n",
    "- https://www.tensorflow.org/\n",
    "\n",
    "To cite my work, point to the URL of the GitHub repository: \n",
    "- https://github.com/guillaume-chevalier/GloVe-as-TensorFlow-Embedding\n",
    "\n",
    "My code is available under the [MIT License](https://github.com/guillaume-chevalier/GloVe-as-TensorFlow-Embedding/blob/master/LICENSE). \n",
    "\n",
    "## Connect with me\n",
    "\n",
    "- https://ca.linkedin.com/in/chevalierg \n",
    "- https://twitter.com/guillaume_che\n",
    "- https://github.com/guillaume-chevalier/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TerminalIPythonApp] WARNING | Subcommand `ipython nbconvert` is deprecated and will be removed in future versions.\n",
      "[TerminalIPythonApp] WARNING | You likely want to use `jupyter nbconvert` in the future\n",
      "[NbConvertApp] Converting notebook GloVe-as-TensorFlow-Embedding-Tutorial.ipynb to markdown\n",
      "[NbConvertApp] Writing 24811 bytes to GloVe-as-TensorFlow-Embedding-Tutorial.md\n"
     ]
    }
   ],
   "source": [
    "# Let's convert this notebook to a README for the GitHub project's title page:\n",
    "!ipython3 nbconvert --to markdown \"GloVe-as-TensorFlow-Embedding-Tutorial.ipynb\"\n",
    "!mv \"GloVe-as-TensorFlow-Embedding-Tutorial.md\" README.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
